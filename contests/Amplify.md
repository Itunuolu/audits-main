## Amplify
[Contest Details](https://audits.sherlock.xyz/contests/1054/report)

### [Issue M-15] Some legitimate `UniV3Decomposer` decompose attempts will always revert due to incorrect liquidity offset calculation

**Summary**

`UniV3Decomposer` takes Uniswap v3 NFT position and converts it into Ammplify Maker position. This is done by burning NFT position into corresponding asset tokens and then calling Maker.newMaker with the same position parameters (pool, tickLower, tickUpper). Position liquidity is slightly reduced compared to original Uniswap NFT position, because:

- burning liquidity rounds down assets received by user
- minting liquidity rounds up assets taken by user
- additionally, when Ammplify Maker position is created, it's minted across multiple ranges to store it in the segment tree. This means that each such mint rounds assets up, so the total reduction of liquidity from the mint assets rounding is multiplied by the number of different segments required to mint the position, which is at most 42

The formula used to calculate liquidity offset is: `liquidity offset = Q96 * 42 / (sqrtPrice(high) - sqrtPrice(low))`.

The issue is that this formula derives the liquidity offset only from `amount1` calculation of the `[a..b]` range, ignoring the effect on `amount0`, as such in some cases there will be enough of `asset1`, but not enough of `asset0` to mint Ammplify Maker liquidity.

Moreover, liquidity offset for each segment minted in the Ammplify `Maker` will be higher than liquidity offset for the `[a..b]` range, so liquidity offset of the `[a..b]` range can't be reliably used to estimate max liquidity offset of sub-segments.

**Detailed Description**
The formula to calculate the amount of token0 and token1 from liquidity is:

- `amount0 = L * Q96 * (b - a) / b / a`
- `amount1 = L * (b - a) / Q96`
where
`L` is liquidity
`b` is upper tick sqrt price
`a` is lower tick sqrt price
`Q96` is `2^96`

When liquidity is burnt, the amounts of assets received are rounded down. When liquidity is minted, assets to be taken from user are rounded up. Due to this, `amount0` (`amount1`) required to mint the same liquidity can be more than `amount0` (`amount1`) received from the burn by at most 1. In order to calculate the liquidity offset (burn and mint liquidity difference), we should reverse the liquidity calculation from amount0 (`amount1`):

- `L[asset0] = ceil(ceil(amount0 * a * b / (b - a)) / Q96)`
- `L[asset1] = ceil(amount1 * Q96 / (b - a))`

Single burn + mint operation results in the loss of at most `amount0 = 1` and `amount1 = 1`. However, since there are multiple segments minted (at most 42), we should use `amount0 = 42` and `amount1 = 42` (so that 42 mint operations, each resulting in a loss of 1 unit of each asset, will combine to at most a loss of 42 units of each asset).

Since we must have enough of both `asset0` and `asset1`, liquidity offset has to be `max(L[asset0], L[asset1])`.

The issue is that in the current code only L[asset1] is used, and additionally it rounds down instead of up.

Moreover, this formula for liquidity offset is only valid for the `[a..b]` range. However, creating new Ammplify `Maker` position will mint up to 42 segments all with different ranges (even though totalling the same `[a..b]` range). In general, each of this segment might require a higher liquidity offset than the `[a..b]` range. This is apparent from researching the properties of the `L[asset0]` and `L[asset1]` surfaces:

- `L[asset0]` increases when `b - a` decreases (the smaller the segment size, the higher the liquidity offset)
- `L[asset1]` increases exponentially when `b - a` decreases
- `L[asset0]` increases quadratically when `a` increases (the greater the left side of the segment, the higher the liquidity offset)
- `L[asset1]` is constant when a increases (if the segment size is the same)

The worst case segment can be derived from these properties: the highest liquidity offset both for `asset0` and `asset1` is in the smallest segment at the rightmost side (`[b - tickSize..b]` segment).

**Root Cause**
Incorrect formula for liquidity offset:
https://github.com/sherlock-audit/2025-09-ammplify/blob/main/Ammplify/src/integrations/UniV3Decomposer.sol#L60-L71

**Internal Pre-conditions**
User trying to decompose liquidity in one of many ranges where conditions specified above are met.

**External Pre-conditions**
None

**Attack Path**
Happens any time user tries to decompose uniswap v3 NFT position with position range described above
Example:

- TickLower = 70020
- TickUpper = 70080

Uniswap V3 NFT position:
- token0 amount used to mint: 1e18
- Liquidity minted: 11064726116504895354478

`UniV3Decomposer`:

- When all this liquidity is burned, `UniV3Decomposer` receives 999999999999999999 token0 (due to rounding down)
- `liquidityOffset = 421`
- `Maker.newMaker` is called with `liquidity = 11064726116504895354057` (421 less than original position liquidity)
- With such liquidity, the amount of token0 which `UniV3Decomposer` must pay is: 1000000000000000000 (1 more than decomposer received from the position burn)
Transaction reverts due to `UniV3Decomposer` not having enough funds to send to `Maker`.

As seen from the example, the liquidity offset for of 421 is not enough. The correct liquidity offset (if calculated from `token0`) is `11065`

**Impact**
Core functionality not working (reverting) in some cases.

**Proof of Concept**
Add this test to `UniV3Decomposer.t.sol`:

```solidity
function testDecomposeNFT_incorrectLiquidityOffset2() public {  
        uint256 pos = createPosition(address(this), 3000, 70020, 70080, 1e18, 1e18);  
        nfpm.setApprovalForAll(address(decomposer), true);  
  
        // Set reasonable price bounds - allowing full range to avoid slippage issues  
        uint160 minSqrtPriceX96 = 4295128739; // Very low price  
        uint160 maxSqrtPriceX96 = 1461446703485210103287273052203988822378723970341; // Very high price  
        decomposer.decompose(pos, false, minSqrtPriceX96, maxSqrtPriceX96, "");  
    }   
```
  
The test will revert with the `FAIL: STF` error message in the transferFrom function:

```solidity
  │   │   │   ├─ [518] MockERC20::transferFrom(UniV3Decomposer: [0xa0Cb889707d426A7A386870A03bc70d1b0697598], SimplexDiamond: [0x2e234DAe75C793f67A35089C9d99245E1C58470b], 1000000000000000000 [1e18])  
    │   │   │   │   └─ ← [Revert] panic: arithmetic underflow or overflow (0x11)  
    │   │   │   └─ ← [Revert] STF  
    │   │   └─ ← [Revert] STF  
    │   └─ ← [Revert] STF  
    └─ ← [Revert] STF 
```
**Mitigation**

Calculate `Liquidity offset` as:

- `L0 = ceil(ceil(42 * a * b / (b - a)) / Q96)`
- `L1 = ceil(42 * Q96 / (b - a))`
- `Liquidity Offset = max(L0, L1)`

Moreover, it's recommended to choose `a = sqrtPrice(upperTick - tickSpacing)` (keeping b the same as now). This will choose the worst case segment in the range. However, in practice it might not be possible to find such a break up into segments that will make the actual transaction revert for multiple factors:

- extremely hard to come up with a range which will mint 42 segments or even 21 segments.
- even in such case, the segments will be of various sizes, not possible to make all of them the smallest size. This means some (smaller) segments migth require 1 more asset, but the other (larger) segments might require less asset due to 42 multiplier, so overall there will be enough assets.
- L[amount0 = 2] for larger (2x) segment ~= L[amount0 = 1] for smaller segment, so the usage of "42" liquidity multiplier might be enough to cover any increase of liquidity offset of smaller segments (L[amount0=42] of large segment > L[amount0=1] of smaller segments). This is not always true as the segment's liquidity offset also depends on a, but for most cases this is true.

